<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Andong Deng's Homepage</title>
  
  <meta name="author" content="Andong Deng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>邓安东 Andong Deng</name>
              </p>
              <p>I am a third year PhD student at <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision</a>, <a href="https://www.ucf.edu/">University of Central Florida</a>, advised by <a href="https://www.crcv.ucf.edu/chenchen/">Dr. Chen Chen</a>.
              </p>
              <p>
                From 2021 to 2022, I have been spending a wonderful period of time at <a href="https://gewu-lab.github.io/">GeWu Lab</a>, working with <a href="https://dtaoo.github.io/">Dr. Di Hu </a>. I have also worked as a deep learning research intern at Big Data Lab, Baidu, working with <a href="https://scholar.google.com/citations?user=f9V0NZkAAAAJ&hl=en"> Dr. Xingjian Li</a> and <a href="https://ix.cs.uoregon.edu/~dou/">Dr. Dejing Dou</a>. Prior to that, I obtained my master's degree from <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a> in 2021, and worked as a research assistant advised by <a href="https://me.sjtu.edu.cn/teacher_directory1/caiweiwei.html">Dr. Weiwei Cai</a>. I spent stupid years from 2016 to 2018, during which I obtained my bachelor's degree from <a href="https://en.scu.edu.cn/">Sichuan University</a> in 2017.
              </p>
              <p>Beyond research, I regularly spend times on photography, hold'em (if you noticed an online poker player called BlackTeaLemon, that's me), music and a variety of sports (especially basketball and surfing).</p>
              <p style="text-align:center">
                <a href="mailto:andong.deng@ucf.edu">Email</a> &nbsp/&nbsp
                <a href="data/AndongDeng_CV.pdf">Curriculum Vitae</a> &nbsp/&nbsp
                <a href="https://twitter.com/dengandong1227">X (Twitter)</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/deng-an-dong-16">知乎 Zhihu</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=FlP80_wAAAAJ">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/pink_circle.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pink_circle.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research Interests</heading>
                <p style="margin-top:8px;">
                  My research focuses on <b>multimodal AI</b>—how we can build models that perceive,
                  ground, and reason over long, complex procedures. I develop <i>benchmarks</i> that stress
                  advanced reasoning, <i>grounding frameworks</i> that align language with space-time, and
                  <i>representation learning</i> methods that scale robustly.
                </p>

                <ul style="margin:2px 0 0 18px; line-height:1.6;">
                  <li>
                    <b>Video Understanding / Reasoning Benchmarks:</b>
                    <a href="#" target="_blank">SciVideoBench(arXiv)</a>,
                    <a href="#" target="_blank">GroundMoRe(CVPR 2025)</a>,
                    <a href="#" target="_blank">Sports-QA(arXiv)</a>
                    <br><span style="font-size:90%; color:#666;">Rigorous evaluations of multimodal reasoning in scientific and real-world domains.</span>
                  </li>

                  <li>
                    <b>Video Spatial / Temporal Grounding Frameworks:</b>
                    <a href="#" target="_blank">Seq2Time(CVPR 2025)</a>,
                    <a href="#" target="_blank">MoRA(CVPR 2025)</a>
                    <br><span style="font-size:90%; color:#666;">Mapping language to precise space-time segments with sequence-level localization.</span>
                  </li>

                  <li>
                    <b>Representation Learning:</b>
                    <a href="#" target="_blank">BEAR(ICCV 2023)</a>,
                    <a href="#" target="_blank">Inadequate Pre-training(ICCV 2023)</a>,
                    <a href="#" target="_blank">OGM-GE(CVPR 2022)</a>,
                    <a href="#" target="_blank">MoPRL(TCSVT 2022)</a>
                    <br><span style="font-size:90%; color:#666;">Foundation features and training recipes for robust visual perception and reasoning.</span>
                  </li>
                </ul>
                <!-- TODO: replace each href="#" with the paper/project URLs -->
              </td>
            </tr>
          </tbody>
        </table>


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Pre-prints</heading>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Updates</heading>
            <p>
              [2025-10-20] Honor to share that SciVideoBench has been selected as the Best Paper Award (Benchmark Track) at <a href="https://knowledgemr-workshop.github.io//">KnowledgeMR Workshop ICCV 2025</a>!
            </p>
            <p>
              [2025-10-10] <a href="https://arxiv.org/abs/2510.08559">SciVideoBench</a> is available now on ArXiv!
            </p>
            <p>
              [2025-10-09] Thrill to announce the first ever scientific video reasoning benchmark <a href="https://scivideobench.github.io/">SciVideoBench</a>!
            </p>
            <p>
              [2025-03-23] I will be joining ByteDance this summer as a research scientist intern!
            </p>
            <p>
              [2025-02-26] Two papers accepted by CVPR 2025! Check <a href="https://arxiv.org/abs/2411.09921">GroundMoRe</a> and <a href="https://arxiv.org/abs/2411.16932">Seq2Time</a>! 
            </p>
            <p>
              [2025-02-21] Invited talk at <a href="https://www.axon.com/">AXON</a> about Video LLMs.
            </p>
            <p>
              [2025-01-25] <a href="https://arxiv.org/pdf/2410.12214">OIS</a> is accepted by ICLR 2025! Congrats to Bin! 
            </p>
            <p>
              [2024-11-25] <a href="https://arxiv.org/abs/2411.16932">Seq2Time</a> is available now on ArXiv! 
            </p>
            <p>
              [2024-11-17] <a href="https://arxiv.org/abs/2411.09921">GroundMoRe</a> is available now on ArXiv! 
            </p>
            <p>
              [2024-06-25] <a href="https://groundmore.github.io/">GroundMoRe</a> will be coming soon! 
            </p>
            <p>
              [2024-05-27] I will be joining UII this summer as a research intern with <a href="https://sites.google.com/site/gaozhongpai/home">Dr. Zhongpai Gao</a> and <a href="https://wuziyan.com/">Dr. Ziyan Wu</a>.
            </p>
            <p>
              [2023-07-14] Two papers accepted by ICCV 2023! 
            </p>
            <p>
              [2023-07-02] One paper accepted by IEEE TCSVT 2023! Congrats to Shoubin and Zhongying!
            </p>
            <p>
              [2023-03-13] One paper accepted by ICME 2023! Congrats to Wenke!
            </p>
            <p>
              [2022-03-09] I will be joining UCF this summer as a CS PhD student with Dr. Chen Chen.
            </p>
            <p>
              [2022-02-03] One paper accepted by CVPR 2022! Congrats to Xiaokang and Yake!
            </p>
          </td>
        </tr>
      </tbody></table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p><sup>*</sup> equal contribution</p>
        </td>
      </tr>
    </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>pre-prints</heading>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/scivideobench.png" alt="scivideobench" width="240" height="160">
          </td>
          <td width="75%" valign="middle">
            <papertitle>SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</papertitle>
            <br>
            <strong>Andong Deng</strong>, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy and Xiaohan Wang
            <br>
            <em>arXiv</em>, 2025
            <br>
            <a href="https://arxiv.org/pdf/2510.08559">arxiv</a>  / <a href="https://scivideobench.github.io/">project</a> 
            <p>We introduce SciVideoBench, the first scientific video reasning benchmark!</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/air.png" alt="AIR" width="240" height="160">
          </td>
          <td width="75%" valign="middle">
            <papertitle>AIR: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering</papertitle>
            <br>
            <strong>Yuanhao Zou, Shengji Jin, Andong Deng</strong>, Youpeng Zhao, Jun Wang, and Chen Chen
            <br>
            <em>arXiv</em>, 2025
            <br>
            <a href="https://arxiv.org/pdf/2510.04428">arxiv</a>  / <a href="https://ucf-air.github.io/">project</a> 
            <p>We introduce AIR, a training-free approach for Adaptive, Iterative, and Reasoning-based frame selection!</p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/sportsqa.png" alt="sportsqa" width="240" height="160">
          </td>
          <td width="75%" valign="middle">
            <papertitle>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</papertitle>
            <br>
            Haopeng Li, <strong>Andong Deng</strong>, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele and Chen Chen
            <br>
            <em>arXiv</em>, 2024
            <br>
            <a href="https://arxiv.org/pdf/2401.01505.pdf">arxiv</a>  / <a href="https://github.com/HopLee6/Sports-QA">code</a> 
            <p>In this paper, we introduce Sports-QA, the first dataset specifically designed for the sports-related VideoQA task. </p>
          </td>
        </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/seq2time.png" alt="seq2time" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding
                </papertitle>
              <br>
              <strong>Andong Deng</strong>, Zhongpai Gao, Anwesa Choudhuri, Benjamin Planche, Meng Zheng, Bin Wang, Terrence Chen, Chen Chen, Ziyan Wu
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2411.16932">arxiv</a>  / <a href="images/Seq2Time_arxiv.pdf">pdf</a> 
              <p>We present Seq2Time, a data-oriented method to enhance the time perception ability of video LLMs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/groundmore.png" alt="groundmore" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level
                </papertitle>
              <br>
              <strong>Andong Deng</strong>, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal and Chen Chen
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2411.09921">arxiv</a>  / <a href="https://groundmore.github.io/">project</a> 
              <p>We present GroundMoRe, a new benchmark for novel Motion-Grounded Video Reasoning, designed to assess multimodal models' reasoning and perception capabilities for motion understanding.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ois.gif" alt="ois" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Order-aware Interactive Segmentation
                </papertitle>
              <br>
              Bin Wang, Anwesa Choudhuri, Meng Zheng, Zhongpai Gao, Benjamin Planche, <strong>Andong Deng</strong>, Qin Liu, Terrence Chen, Ulas Bagci and Ziyan Wu.
              <br>
              <em>ICLR</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2410.12214">arxiv</a>  / <a href="https://ukaukaaaa.github.io/projects/OIS">project</a> 
              <p>We propose OIS: order-aware interactive segmentation, to explicitly integrate missing relative depth information into 2D interactive segmentation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BEAR_teaser.jpg" alt="inadequate" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition</papertitle>
              <br>
              <strong>Andong Deng<sup>*</sup></strong>, Taojiannan Yang<sup>*</sup>, and Chen Chen
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.13505.pdf">arxiv</a>  / <a href="https://github.com/AndongDeng/BEAR">code</a> / <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_A_Large-scale_Study_of_Spatiotemporal_Representation_Learning_with_a_New_ICCV_2023_paper.pdf">CVF</a> / <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Deng_A_Large-scale_Study_ICCV_2023_supplemental.pdf">Supp</a> 
              <p>We introduce BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18 video datasets grouped into 5 categories, which covers a diverse set of real-world applications.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/inadequate_pretrained.png" alt="inadequate" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Towards Inadequately Pre-trained Models in Transfer Learning</papertitle>
              <br>
              <strong>Andong Deng<sup>*</sup></strong>, Xingjian Li<sup>*</sup>, Di Hu, Tianyang Wang, Haoyi Xiong and Chengzhong Xu
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2203.04668.pdf">arxiv</a> / <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Towards_Inadequately_Pre-trained_Models_in_Transfer_Learning_ICCV_2023_paper.pdf">CVF</a> / <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Deng_Towards_Inadequately_Pre-trained_ICCV_2023_supplemental.pdf">Supp</a> 
              <p>Inadequately pre-trained models often extract better features than fully pre-trained ones. And deep models tend to first learn spectral components corresponding to large singular values.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/moprl_net.png" alt="moprl" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Regularity Learning via Explicit Distribution Modeling for Skeletal Video Anomaly Detection</papertitle>
              <br>
              Shoubin Yu, Zhongying Zhao, Haoshu Fang, <strong>Andong Deng</strong>, Haisheng Su, Dongliang Wang, Weihao Gan, Cewu Lu and Wei Wu
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.03649.pdf">arxiv</a>
              <p>A motion prior distribution is utilized to construct the pose representation. MoPRL achieves the state-of-the-art performance by an average improvement of 4.7% AUC on several challenging datasets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wenke_icme.png" alt="wenke_icme" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robust Cross-Modal Knowledge Distillation for Unconstrained Videos</papertitle>
              <br>
              Wenke Xia, Xingjian Li, <strong>Andong Deng</strong>, Haoyi Xiong, Dejing Dou, and Di Hu
              <br>
              <em>ICME</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2304.07775.pdf">arxiv</a>
              <p>We propose a Modality Noise Filter module to erase the irrelevant noise in teacher modality with cross-modal context and design a Contrastive Semantic Calibration module to adaptively distill useful knowledge for target modality, by referring to the differentiated sample-wise semantic correlation in a contrastive fashion.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ogmge.png" alt="ogm-ge" width="240" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Balanced Multimodal Learning via On-the-fly Gradient Modulation</papertitle>
              <br>
              Xiaokang Peng<sup>*</sup>, Yake Wei<sup>*</sup>, <strong>Andong Deng</strong>, Dong Wang and Di Hu
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arxiv</a> / <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Peng_Balanced_Multimodal_Learning_via_On-the-Fly_Gradient_Modulation_CVPR_2022_paper.pdf">CVF</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code</a>
              <p>Modulate gradients of two modalities to adaptively balance the optimization process of multimodal learning.</p>
            </td>
          </tr>


        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
              <p>
                <strong>Conference Reviewer</strong>: CVPR, ECCV, ICCV, ACCV
              <p>
              <p>
                <strong>Journal Reviewer</strong>: IEEE IoT, IEEE TNNLS, IJCV, IEEE TAPMI
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Gallery</heading>
          </td>
          </tr>
        </tbody></table>

        <!DOCTYPE html>
        <html>
        <head>
        <style>
          .gallery {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            grid-gap: 20px;
            width: 100%;
            max-width: 1200px;
            margin: auto;
          }
          .gallery img {
            width: 100%;
            height: auto;
            border-radius: 8px; /* Optional for rounded corners */
            transition: transform 0.3s ease; /* Optional for hover effect */
          }
          .gallery img:hover {
            transform: scale(1.05); /* Optional for hover effect */
          }
        </style>
        </head>
        <body>

        <div class="gallery">
          <img src="gallery/yongman.jpg" alt="Photo 21">
          <img src="gallery/blackwhitebird.jpg" alt="Photo 20">
          <img src="gallery/fishingman.jpg" alt="Photo 19">
          <img src="gallery/whale.jpg" alt="Photo 18">
          <img src="gallery/seagull.jpg" alt="Photo 17">
          <img src="gallery/sunrisebird.jpg" alt="Photo 16">
          <img src="gallery/my cute man and the stupid seagull.jpg" alt="Photo 1">
          <img src="gallery/greenbird.jpg" alt="Photo 11">
          <img src="gallery/golden.jpg" alt="Photo 3">
          <img src="gallery/waves.jpg" alt="Photo 2">
          <img src="gallery/rainbow in the shadow.jpg" alt="Photo 4">
          <img src="gallery/skysea.jpg" alt="Photo 5">
          <img src="gallery/bluemoon.jpg" alt="Photo 6">
          <img src="gallery/regularmoon.jpg" alt="Photo 7">
          <img src="gallery/cinimaposter.jpg" alt="Photo 8">
          <img src="gallery/father&son.jpg" alt="Photo 9">
          <img src="gallery/gator.jpg" alt="Photo 10">
          <img src="gallery/paintedjet.jpg" alt="Photo 12">
          <img src="gallery/whitebird.jpg" alt="Photo 13">
          <img src="gallery/sunonbird.jpg" alt="Photo 14">
          <img src="gallery/spacex.jpg" alt="Photo 15">
        </div>
          
        </body>
        </html>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                Updated: March 23th, 2025.
              </p>
            </td>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks <a href="https://jonbarron.info/">Jon Barron</a> for this amazing template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
